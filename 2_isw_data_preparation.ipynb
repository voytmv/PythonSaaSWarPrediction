{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f249234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a01ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/0_isw_data_collection/1_isw_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12f78d",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab214074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c098365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664ebc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdfb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy \n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be45103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>short_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text_title</th>\n",
       "      <th>full_url</th>\n",
       "      <th>main_html</th>\n",
       "      <th>main_html_v6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>RusCampaignFeb25</td>\n",
       "      <td>Russia-Ukraine Warning Update: Russian Offensi...</td>\n",
       "      <td>Russia-Ukraine Warning Update: Russian Offensi...</td>\n",
       "      <td>/backgrounder/russia-ukraine-warning-update-ru...</td>\n",
       "      <td>&lt;div class=\"field field-name-body field-type-t...</td>\n",
       "      <td>Mason Clark, George Barros, and Kateryna Step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-26</td>\n",
       "      <td>RusCampaignFeb26</td>\n",
       "      <td>Russia-Ukraine Warning Update: Russian Offensi...</td>\n",
       "      <td>Russia-Ukraine Warning Update: Russian Offensi...</td>\n",
       "      <td>/backgrounder/russia-ukraine-warning-update-ru...</td>\n",
       "      <td>&lt;div class=\"field field-name-body field-type-t...</td>\n",
       "      <td>Mason Clark, George Barros, and Katya Stepane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-02-27</td>\n",
       "      <td>RusCampaignFeb27</td>\n",
       "      <td>Russia-Ukraine Warning Update: Russian Offensi...</td>\n",
       "      <td>Russia-Ukraine Warning Update: Russian Offensi...</td>\n",
       "      <td>/backgrounder/russia-ukraine-warning-update-ru...</td>\n",
       "      <td>&lt;div class=\"field field-name-body field-type-t...</td>\n",
       "      <td>Mason Clark, George Barros, and Kateryna Step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>RusCampaignFeb28</td>\n",
       "      <td>Russian Offensive Campaign Assessment, Februar...</td>\n",
       "      <td>Russian Offensive Campaign Assessment, Februar...</td>\n",
       "      <td>/backgrounder/russian-offensive-campaign-asses...</td>\n",
       "      <td>&lt;div class=\"field field-name-body field-type-t...</td>\n",
       "      <td>Mason Clark, George Barros, and Kateryna Step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>RusCampaignMar1</td>\n",
       "      <td>Russian Offensive Campaign Assessment, March 1...</td>\n",
       "      <td>Russian Offensive Campaign Assessment, March 1</td>\n",
       "      <td>/backgrounder/russian-offensive-campaign-asses...</td>\n",
       "      <td>&lt;div class=\"field field-name-body field-type-t...</td>\n",
       "      <td>Frederick W. Kagan, George Barros, and Katery...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date         short_url  \\\n",
       "0           0  2022-02-25  RusCampaignFeb25   \n",
       "1           1  2022-02-26  RusCampaignFeb26   \n",
       "2           2  2022-02-27  RusCampaignFeb27   \n",
       "3           3  2022-02-28  RusCampaignFeb28   \n",
       "4           4  2022-03-01   RusCampaignMar1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Russia-Ukraine Warning Update: Russian Offensi...   \n",
       "1  Russia-Ukraine Warning Update: Russian Offensi...   \n",
       "2  Russia-Ukraine Warning Update: Russian Offensi...   \n",
       "3  Russian Offensive Campaign Assessment, Februar...   \n",
       "4  Russian Offensive Campaign Assessment, March 1...   \n",
       "\n",
       "                                          text_title  \\\n",
       "0  Russia-Ukraine Warning Update: Russian Offensi...   \n",
       "1  Russia-Ukraine Warning Update: Russian Offensi...   \n",
       "2  Russia-Ukraine Warning Update: Russian Offensi...   \n",
       "3  Russian Offensive Campaign Assessment, Februar...   \n",
       "4     Russian Offensive Campaign Assessment, March 1   \n",
       "\n",
       "                                            full_url  \\\n",
       "0  /backgrounder/russia-ukraine-warning-update-ru...   \n",
       "1  /backgrounder/russia-ukraine-warning-update-ru...   \n",
       "2  /backgrounder/russia-ukraine-warning-update-ru...   \n",
       "3  /backgrounder/russian-offensive-campaign-asses...   \n",
       "4  /backgrounder/russian-offensive-campaign-asses...   \n",
       "\n",
       "                                           main_html  \\\n",
       "0  <div class=\"field field-name-body field-type-t...   \n",
       "1  <div class=\"field field-name-body field-type-t...   \n",
       "2  <div class=\"field field-name-body field-type-t...   \n",
       "3  <div class=\"field field-name-body field-type-t...   \n",
       "4  <div class=\"field field-name-body field-type-t...   \n",
       "\n",
       "                                        main_html_v6  \n",
       "0   Mason Clark, George Barros, and Kateryna Step...  \n",
       "1   Mason Clark, George Barros, and Katya Stepane...  \n",
       "2   Mason Clark, George Barros, and Kateryna Step...  \n",
       "3   Mason Clark, George Barros, and Kateryna Step...  \n",
       "4   Frederick W. Kagan, George Barros, and Katery...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb6503",
   "metadata": {},
   "source": [
    "### Prepare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_one_letter_word(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    \n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if (len(w) > 1):\n",
    "            new_text = new_text + \" \" + w\n",
    "            \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_one_letter_word(data['main_html_v6'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ff784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb27064",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_stop_words = {\"no\",\"not\"}\n",
    "    \n",
    "    stop_words = stop_words - stop_stop_words\n",
    "    \n",
    "    words = word_tokenize(str(data))\n",
    "    \n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "            \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_stop_words(data['main_html_v6'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c930ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(data):\n",
    "    symbols = \" !()-[]{};:'\\,<>./?@#$%^&*_~ '\"\"\"  \n",
    "    \n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data,symbols[i],\" \")\n",
    "        data = np.char.replace(data,\"  \",\" \")\n",
    "    \n",
    "    data = np.char.replace(data,\",\", \" \")\n",
    "    \n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_punctuations(data['main_html_v6'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def lemmatizing(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "               \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32feaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        if w.isdigit():\n",
    "            if int(w) < 1000000000:\n",
    "                w = num2words(w)\n",
    "            else:\n",
    "                w = \" \"\n",
    "        new_text = new_text + \" \" + w\n",
    "            \n",
    "    new_text = np.char.replace(new_text,\"-\",\" \")\n",
    "        \n",
    "    return new_text\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_numbers(data['main_html_v6'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_from_string(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "\n",
    "    \n",
    "    for w in words:\n",
    "        w = re.sub(r'https?:\\/\\/.*[\\r\\n]*',\"\",str(w),flags=re.MULTILINE)\n",
    "        w = re.sub(r'http?:\\/\\/.*[\\r\\n]*',\"\",str(w),flags=re.MULTILINE)\n",
    "        \n",
    "        new_text = new_text + \" \" + w\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85522bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, word_root_algo='lemm'):\n",
    "    \n",
    "    data = remove_one_letter_word(data)\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_url_from_string(data)\n",
    "    \n",
    "    if word_root_algo =='lemm':\n",
    "        data = lemmatizing(data)\n",
    "    else: \n",
    "        data = stemming(data)\n",
    "        \n",
    "    \n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_stop_words(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ebcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['report_text_lemm'] = data['main_html_v6'].apply(lambda x: preprocess(x,\"lemm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['report_text_stemm'] = data['main_html_v6'].apply(lambda x: preprocess(x,\"stemm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62302ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099cfb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['report_text_lemm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9241e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data['report_text_lemm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/0_isw_data_collection/2_isw_preprocessed_before_vectosing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5667f3",
   "metadata": {},
   "source": [
    "## CountVerctorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.98,min_df=2)\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d24bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/count_vectorizer_v1.pkl\",\"wb\") as handle:\n",
    "    pickle.dump(cv, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d3ccb",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/tfidf_transformer_v1.pkl\",\"wb\") as handle:\n",
    "    pickle.dump(tfidf_transformer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names_out(),columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4677780",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176363b",
   "metadata": {},
   "source": [
    "### Transformation with fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open(\"models/tfidf_transformer_v1.pkl\",\"rb\"))\n",
    "cv = pickle.load(open(\"models/count_vectorizer_v1.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b325fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23334f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    \n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    \n",
    "    return sorted(tuples,key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        score_vals.append(round(score,3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "        \n",
    "    results = {}\n",
    "    \n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_doc_to_vector(doc):\n",
    "    feature_names = cv.get_feature_names_out()\n",
    "    top_n = 10\n",
    "    tf_idf_vector = tfidf.transform(cv.transform([doc]))\n",
    "    \n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    \n",
    "    keywords = extract_topn_from_vector(feature_names, sorted_items, top_n)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords'] = data['report_text_stemm'].apply(lambda x: convert_doc_to_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4102f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorised = tf_idf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_df = pd.DataFrame(data_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba80ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_df.to_csv('data/0_isw_data_collection/3_isw_vectorised.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
